{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/serikoo/AgreementProbing-1\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(example):\n",
    "    idx = 1\n",
    "    enc =[(x, tokenizer.encode(x, add_special_tokens=False)) for x in example.split()]\n",
    "\n",
    "    desired_output = []\n",
    "\n",
    "    for x, token in enc:\n",
    "        tokenoutput = []\n",
    "        for ids in token:\n",
    "            tokenoutput.append(idx)\n",
    "            idx +=1\n",
    "        desired_output.append(tokenoutput)\n",
    "\n",
    "    return desired_output\n",
    "\n",
    "def get_attention_scores(\n",
    "    sentence:str,\n",
    "    attention,\n",
    "    subj:int = 0,\n",
    "    obj:int = 2,\n",
    "    verb_b:int = 3,\n",
    "    verb_e:int = 5\n",
    "):\n",
    "    words = decode_tokens(sentence)\n",
    "    subj_word = words[subj]\n",
    "    obj_word = words[obj]\n",
    "    verb = [idx for x in words[verb_b:verb_e] for idx in x]\n",
    "\n",
    "    subj_attention = attention[:,verb[0]:verb[-1]+1,subj_word[0]:subj_word[-1]+1].mean(axis=[1,2])\n",
    "    obj_attention = attention[:,verb[0]:verb[-1]+1,obj_word[0]:obj_word[-1]+1].mean(axis=[1,2])\n",
    "\n",
    "    return subj_attention, obj_attention\n",
    "\n",
    "def return_batch_attention(sentences, attentions, emb_size=12):\n",
    "    subj_att_weights = torch.zeros(len(sentences), emb_size)\n",
    "    obj_att_weights = torch.zeros(len(sentences), emb_size)\n",
    "    for num, (sentence, attention) in enumerate(zip(sentences, attentions)):\n",
    "        subj_W, obj_W = get_attention_scores(sentence, attention)\n",
    "        subj_att_weights[num] = subj_W\n",
    "        obj_att_weights[num] = obj_W\n",
    "    return subj_att_weights, obj_att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:39,  9.77s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 19.70 GiB total capacity; 6.17 GiB already allocated; 83.19 MiB free; 6.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     MODEL_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mai-forever/ruGPT-3.5-13B\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     model, tokenizer, config \u001b[39m=\u001b[39m load_model_tokenizer_config(MODEL_NAME, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/AgreementProbing-1/utils.py:7\u001b[0m, in \u001b[0;36mload_model_tokenizer_config\u001b[0;34m(model_name, output_attentions)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_tokenizer_config\u001b[39m(model_name\u001b[39m=\u001b[39mMODEL_NAME, output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m         model_name,\n\u001b[1;32m      9\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m         offload_folder\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./_tmp\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         max_memory\u001b[39m=\u001b[39;49m{\u001b[39m0\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m14GB\u001b[39;49m\u001b[39m'\u001b[39;49m},\n\u001b[1;32m     13\u001b[0m         cache_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.cache/huggingface\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     17\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3706\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3697\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3698\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3699\u001b[0m     (\n\u001b[1;32m   3700\u001b[0m         model,\n\u001b[1;32m   3701\u001b[0m         missing_keys,\n\u001b[1;32m   3702\u001b[0m         unexpected_keys,\n\u001b[1;32m   3703\u001b[0m         mismatched_keys,\n\u001b[1;32m   3704\u001b[0m         offload_index,\n\u001b[1;32m   3705\u001b[0m         error_msgs,\n\u001b[0;32m-> 3706\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3707\u001b[0m         model,\n\u001b[1;32m   3708\u001b[0m         state_dict,\n\u001b[1;32m   3709\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3710\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3711\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3712\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3713\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3714\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3715\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3716\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3717\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3718\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3719\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3720\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3721\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3722\u001b[0m     )\n\u001b[1;32m   3724\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3725\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/transformers/modeling_utils.py:4116\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4112\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4113\u001b[0m                         model_to_load, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mempty(\u001b[39m*\u001b[39mparam\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   4114\u001b[0m                     )\n\u001b[1;32m   4115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4116\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4117\u001b[0m             model_to_load,\n\u001b[1;32m   4118\u001b[0m             state_dict,\n\u001b[1;32m   4119\u001b[0m             loaded_keys,\n\u001b[1;32m   4120\u001b[0m             start_prefix,\n\u001b[1;32m   4121\u001b[0m             expected_keys,\n\u001b[1;32m   4122\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4123\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4124\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4125\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4126\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4127\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4128\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   4129\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4130\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4131\u001b[0m         )\n\u001b[1;32m   4132\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4133\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/transformers/modeling_utils.py:786\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    783\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 786\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    787\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    788\u001b[0m             )\n\u001b[1;32m    790\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:96\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     94\u001b[0m kwargs \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m is_8bit:\n\u001b[0;32m---> 96\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mInt8Params(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     97\u001b[0m \u001b[39melif\u001b[39;00m is_4bit:\n\u001b[1;32m     98\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:344\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\n\u001b[1;32m    336\u001b[0m     \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    340\u001b[0m     device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m ):\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     new_param \u001b[39m=\u001b[39m Int8Params(\n\u001b[1;32m    347\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\n\u001b[1;32m    348\u001b[0m             device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype, non_blocking\u001b[39m=\u001b[39mnon_blocking\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m         has_fp16_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    352\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:308\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[39m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mhalf()\u001b[39m.\u001b[39mcuda(device)\n\u001b[0;32m--> 308\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mdouble_quant(B)\n\u001b[1;32m    309\u001b[0m     \u001b[39mdel\u001b[39;00m CBt\n\u001b[1;32m    310\u001b[0m     \u001b[39mdel\u001b[39;00m SCBt\n",
      "File \u001b[0;32m~/miniconda3/envs/agreement-env/lib/python3.10/site-packages/bitsandbytes/functional.py:2121\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     out_col \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(A\u001b[39m.\u001b[39mshape, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint8)\n\u001b[1;32m   2120\u001b[0m \u001b[39mif\u001b[39;00m out_row \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     out_row \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(A\u001b[39m.\u001b[39;49mshape, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint8)\n\u001b[1;32m   2123\u001b[0m coo_tensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m ptrA \u001b[39m=\u001b[39m get_ptr(A)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 19.70 GiB total capacity; 6.17 GiB already allocated; 83.19 MiB free; 6.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from utils import load_model_tokenizer_config\n",
    "\n",
    "BERT=False\n",
    "if BERT:\n",
    "    #MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "    #MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "    MODEL_NAME = 'microsoft/mdeberta-v3-base'\n",
    "\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME, output_attentions=True)\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "else:\n",
    "    MODEL_NAME = 'ai-forever/ruGPT-3.5-13B'\n",
    "    model, tokenizer, config = load_model_tokenizer_config(MODEL_NAME, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = pd.read_csv(\"stimuli.csv\")\n",
    "sentences = stimuli.Sentence.to_list()\n",
    "data_loader = torch.utils.data.DataLoader(sentences, batch_size=4)\n",
    "device = \"cpu\"\n",
    "subj_predictions = []\n",
    "obj_predictions = []\n",
    "for sent_batch in tqdm(data_loader):\n",
    "    enc = tokenizer(sent_batch, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        enc = enc.to(device)\n",
    "        output = model(**enc, return_dict=True)\n",
    "        subj, obj = return_batch_attention(sent_batch, output[\"attentions\"][-1])\n",
    "        subj_predictions.append(subj)\n",
    "        obj_predictions.append(obj)\n",
    "subj_attention = torch.cat(subj_predictions)\n",
    "obj_attention = torch.cat(obj_predictions)\n",
    "sss = stimuli.index[stimuli['Code'] == 'S_S-S'].tolist()\n",
    "pss = stimuli.index[stimuli['Code'] == 'P_S-S'].tolist()\n",
    "ssp = stimuli.index[stimuli['Code'] == 'S_S-P'].tolist()\n",
    "sps = stimuli.index[stimuli['Code'] == 'S_P-S'].tolist()\n",
    "pps = stimuli.index[stimuli['Code'] == 'P_P-S'].tolist()\n",
    "spp = stimuli.index[stimuli['Code'] == 'S_P-P'].tolist()\n",
    "ppp = stimuli.index[stimuli['Code'] == 'P_P-P'].tolist()\n",
    "psp = stimuli.index[stimuli['Code'] == 'P_S-P'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bertviz import model_view, head_view\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".3\", \"grid.linestyle\": \":\"})\n",
    "sns.color_palette(\"mako\", as_cmap=True)\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(20, 15), sharey=True)\n",
    "\n",
    "type2title_and_ax = [\n",
    "    (sss, \"S_S-S\", ax[0,0]),\n",
    "    (ssp, \"S_S-P\", ax[0,1]),\n",
    "    (sps, \"S_P-S\", ax[1,0]),\n",
    "    (spp, \"S_P-P\", ax[1,1]),\n",
    "    (psp, \"P_S-P\", ax[2,0]),\n",
    "    (pss, \"P_S-S\", ax[2,1]),\n",
    "    (ppp, \"P_P-P\", ax[3,0]),\n",
    "    (pps, \"P_P-S\", ax[3,1])\n",
    "]\n",
    "\n",
    "for (type, title, ax_) in type2title_and_ax:\n",
    "    sns.lineplot(subj_attention[type].mean(axis=0), ax=ax_, color=\"green\")\n",
    "    sns.lineplot(obj_attention[type].mean(axis=0), ax=ax_, color=\"purple\")\n",
    "    ax_.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "cases = {title: type_ for type_, title, _ in type2title_and_ax}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "names = []\n",
    "for case_1, case_2 in combinations(cases.items(), 2):\n",
    "    res = ttest_rel(subj_attention[case_1[1]].mean(0), subj_attention[case_2[1]].mean(0))\n",
    "    pvals.append(res.pvalue)\n",
    "    names.append(f\"{case_1[0]} - {case_2[0]}\")\n",
    "\n",
    "pvals = multipletests(pvals, method=\"bonferroni\")\n",
    "import pandas as pd\n",
    "\n",
    "subj_attention = pd.DataFrame({\"comparison\": names, \"p-value\": pvals[1], \"significant\": pvals[0]})\n",
    "subj_attention.to_csv(f\"{MODEL_NAME.split('/')[-1]}_subj_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "names = []\n",
    "for case_1, case_2 in combinations(cases.items(), 2):\n",
    "    res = ttest_rel(obj_attention[case_1[1]].mean(0), obj_attention[case_2[1]].mean(0))\n",
    "\n",
    "    pvals.append(res.pvalue)\n",
    "    names.append(f\"{case_1[0]} - {case_2[0]}\")\n",
    "pvals = multipletests(pvals, method=\"bonferroni\")\n",
    "obj_attention = pd.DataFrame({\"comparison\": names, \"p-value\": pvals[1], \"significant\": pvals[0]})\n",
    "obj_attention.to_csv(f\"{MODEL_NAME.split('/')[-1]}_obj_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[0]\n",
    "\n",
    "enc = tokenizer(sentence, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    enc = enc.to(device)\n",
    "    output = model(**enc, return_dict=True)\n",
    "\n",
    "attention = output[\"attentions\"]\n",
    "tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])  # Convert input ids to token strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrporb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
